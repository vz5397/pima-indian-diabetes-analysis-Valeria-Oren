# -*- coding: utf-8 -*-
"""pima-indians-diabetes-databaseO&V.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y0rRZsUTxGcgV4o4G_-OfJb2PWO6RfFM

/content/1_3Zzg_xcOek5dK_MfrCItCg.png

Oren Grabois & Valeriya Gamerman

# Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high.
# Blood glucose is your main source of energy and comes from the food you eat.
# Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy.


# ***# Early Signs of Diabetes***


#1-Hunger and fatigue.
#2-Your body converts the food you eat into glucose that your cells use for energy

#3-Peeing more often and being thirstier
#4-Dry mouth and itchy skin
#5-Blurred vision

# ***Variables: :-***

#*Pregnancies: Number of times pregnant
#*Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
#*BloodPressure: Diastolic blood pressure (mm Hg)
#*SkinThickness: Triceps skin fold thickness (mm)
#*Insulin: 2-Hour serum insulin (mu U/ml)
#*BMI: Body mass index (weight in kg/(height in m)^2)
#*DiabetesPedigreeFunction: Diabetes pedigree functionr
#*Age: Age (years)
#*Cabin : Cabin Number
#*Outcome: Class variable (0 or 1)
"""

from google.colab import files
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
uploaded = files.upload()

import io
import matplotlib.pyplot as plt
data = pd.read_csv(io.BytesIO(uploaded['diabetes.csv']))
data.head()

data.isnull().values.any() #check if any null value is present
#data.isna().sum()- onother option

import seaborn as sns
#get correlations of each features in dataset
corrmat = data.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap="BuPu")

data.corr()

# Train Test Split

from sklearn.model_selection import train_test_split
feature_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
predicted_class = ['Outcome']

data.head()

X = data[feature_columns].values
y = data[predicted_class].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.50, random_state=10)

print("total number of rows : {0}".format(len(data)))
print("number of rows missing Pregnancies: {0}".format(len(data.loc[data['Pregnancies'] == 0])))
print("number of rows missing Glucose: {0}".format(len(data.loc[data['Glucose'] == 0])))
print("number of rows missing BloodPressure: {0}".format(len(data.loc[data['BloodPressure'] == 0])))
print("number of rows missing SkinThickness: {0}".format(len(data.loc[data['SkinThickness'] == 0])))
print("number of rows missing Insulin: {0}".format(len(data.loc[data['Insulin'] == 0])))
print("number of rows missing BMI: {0}".format(len(data.loc[data['BMI'] == 0])))
print("number of rows missing DiabetesPedigreeFunction: {0}".format(len(data.loc[data['DiabetesPedigreeFunction'] == 0])))
print("number of rows missing Age: {0}".format(len(data.loc[data['Age'] == 0])))

#checking outliers
def Remove_Outlier (feature_columns):
    Q1,Q3 = np.percentile (feature_columns,[25,75])
    
    IQR= Q3-Q1
    
    upper_range =  Q3+(IQR*1.5)
    
    lower_range =  Q1-(IQR*1.5)
    
    return upper_range,lower_range

print("Shape Of The Before Ouliers: ", data.shape)

for i in data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']]:
    ur,lr = Remove_Outlier(data[i])
    data[i]= np.where(data[i]>ur,ur,data[i])
    data[i]= np.where(data[i]<lr,lr,data[i])

print("Shape Of The After Ouliers: ", data.shape)

from sklearn.impute import SimpleImputer

fill_values = SimpleImputer(missing_values=0, strategy="mean")
X_train = fill_values.fit_transform(X_train)
X_test = fill_values.fit_transform(X_test)

from sklearn.ensemble import RandomForestClassifier
random_forest_model = RandomForestClassifier(random_state=100)

random_forest_model.fit(X_train, y_train.ravel())

predict_train_data = random_forest_model.predict(X_test)

from sklearn import metrics

print("Accuracy = {0:.3f}".format(metrics.accuracy_score(y_test, predict_train_data)))

# The random forest hyperparameters structure
#n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]
# Number of features to consider at every split
#max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
#max_depth = [2,4]
# Minimum number of samples required to split a node
#min_samples_split = [2, 5]
# Minimum number of samples required at each leaf node
#min_samples_leaf = [1, 2]
# Method of selecting samples for training each tree
#bootstrap = [True, False]

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [2,4]
# Minimum number of samples required to split a node
min_samples_split = [2, 5]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2]
# Method of selecting samples for training each tree
bootstrap = [True, False]

# Create the param grid using dictinary
param_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
print(param_grid )

rf_Model = RandomForestClassifier()

## Hyperparameter optimization using RandomizedSearchCV
from sklearn.model_selection import GridSearchCV

rf_Grid = GridSearchCV(estimator = rf_Model, param_grid = param_grid, cv = 4, verbose=2, n_jobs = 4)
#cv: number of cross-validation you have to try for each selected set of hyperparameters 
#verbose: you can set it to 1 to get the detailed print out while you fit the data
#n_jobs: number of processes you wish to run in parallel

rf_Grid.fit(X_train,y_train)

rf_Grid.best_params_

print (f'Train Accuracy is : {rf_Grid.score(X_train,y_train):.3f}')
print (f'Test Accuracy is : {rf_Grid.score(X_test,y_test):.3f}')

#getting score using knn method
from sklearn.neighbors import KNeighborsClassifier
knn= KNeighborsClassifier(n_neighbors=9)
knn.fit(X_train,y_train.ravel())
from sklearn.metrics import accuracy_score,confusion_matrix

print("Train Set Accuracy:"+str(accuracy_score(y_train.ravel(),knn.predict(X_train))*100))
print("Test Set Accuracy:"+str(accuracy_score(y_test.ravel(),knn.predict(X_test))*100))

from sklearn import svm
from sklearn.metrics import accuracy_score

classifier = svm.SVC(kernel='linear')
#training the support vector Machine Classifier
classifier.fit(X_train, y_train)
# accuracy score on the training data
X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, y_train)
# accuracy score on the test data
X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, y_test)
print('Accuracy score of the test data : ', test_data_accuracy)
input_data = (1,85,66,29,0,26.6,0.351,31)

# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = classifier.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print('The person is not diabetic')
else:
  print('The person is diabetic')

# accuracy score on the training data
X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, y_train)
# accuracy score on the test data
X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, y_test)
print('Accuracy score of the training data : ', training_data_accuracy)
print('Accuracy score of the test data : ', test_data_accuracy)

np.size(predictions)

for i in data.columns:
    if i!='Outcome':
        sns.boxplot(x = data["Outcome"],y=data[i])
        plt.show()

plt.figure(figsize=(20,20
                   ))
sns.pairplot(data,hue = "Outcome")

data.describe()

data.var()

sns.countplot(data["Outcome"])

#Outcome= pd.DataFrame({'Actual': y_test, 'Predicted': y_train}.reset_index())
#Outcome